import pandas_datareader as pdr
import pandas as pd
key = "ec9f337c06668b4bd2e04bbaf560780edb4d2904"
df = pdr.get_data_tiingo('AMZN', api_key=key)
df.to_csv('AMZN.csv')
# pandas
df = pd.read_csv('AMZN.csv')
df.head()
symbol	date	close	high	low	open	volume	adjClose	adjHigh	adjLow	adjOpen	adjVolume	divCash	splitFactor
0	AMZN	2016-11-22 00:00:00+00:00	785.33	792.40	781.00	788.17	5311320	785.33	792.40	781.00	788.17	5311320	0.0	1.0
1	AMZN	2016-11-23 00:00:00+00:00	780.12	781.75	773.12	781.73	3540263	780.12	781.75	773.12	781.73	3540263	0.0	1.0
2	AMZN	2016-11-25 00:00:00+00:00	780.37	786.75	777.90	786.50	1837068	780.37	786.75	777.90	786.50	1837068	0.0	1.0
3	AMZN	2016-11-28 00:00:00+00:00	766.77	777.00	764.24	776.99	4438828	766.77	777.00	764.24	776.99	4438828	0.0	1.0
4	AMZN	2016-11-29 00:00:00+00:00	762.52	769.89	761.32	768.00	3272344	762.52	769.89	761.32	768.00	3272344	0.0	1.0
df.tail()
symbol	date	close	high	low	open	volume	adjClose	adjHigh	adjLow	adjOpen	adjVolume	divCash	splitFactor
1253	AMZN	2021-11-15 00:00:00+00:00	3545.68	3593.880	3525.8100	3537.00	2929719	3545.68	3593.880	3525.8100	3537.00	2929719	0.0	1.0
1254	AMZN	2021-11-16 00:00:00+00:00	3540.70	3576.500	3525.1466	3539.00	2217071	3540.70	3576.500	3525.1466	3539.00	2217071	0.0	1.0
1255	AMZN	2021-11-17 00:00:00+00:00	3549.00	3587.245	3545.3500	3564.72	2560270	3549.00	3587.245	3545.3500	3564.72	2560270	0.0	1.0
1256	AMZN	2021-11-18 00:00:00+00:00	3696.06	3704.200	3561.0000	3566.35	5703538	3696.06	3704.200	3561.0000	3566.35	5703538	0.0	1.0
1257	AMZN	2021-11-19 00:00:00+00:00	3676.57	3762.145	3675.7200	3712.69	4946203	3676.57	3762.145	3675.7200	3712.69	4946203	0.0	1.0
df1 = df.reset_index()['close']
df1
0        785.33
1        780.12
2        780.37
3        766.77
4        762.52
         ...   
1253    3545.68
1254    3540.70
1255    3549.00
1256    3696.06
1257    3676.57
Name: close, Length: 1258, dtype: float64
df1.shape
(1258,)
import matplotlib.pyplot as plt
import numpy as np
plt.plot(df1)
[<matplotlib.lines.Line2D at 0x20dcf1be520>]

# LSTM are sensitive to the scale of the data. so we apply MinMax scaler

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0,1))
df1 = scaler.fit_transform(np.array(df1).reshape(-1,1))
print(df1)
[[0.01504144]
 [0.01329959]
 [0.01338317]
 ...
 [0.93901514]
 [0.98818149]
 [0.98166542]]
#splitting dataset into train and test split

training_size = int(len(df1) * 0.65)
test_size = len(df1) - training_size
train_data, test_data = df1[0:training_size,:], df1[training_size:len(df1), :1]
training_size, test_size
(817, 441)
# convert an array of values into a dataset matrix
def create_dataset(dataset, time_step = 1):
	dataX, dataY = [], []
	for i in range( len(dataset) - time_step - 1 ):
		a = dataset[i : (i + time_step), 0]   ###i=0, 0,1,2,3-----99   100 
		dataX.append(a)
		dataY.append(dataset[i + time_step, 0])
	return np.array(dataX), np.array(dataY)
# reshape into X=t,t+1,t+2,t+3 and Y=t+4
time_step = 100
X_train, y_train = create_dataset(train_data, time_step)
X_test, ytest = create_dataset(test_data, time_step)
print(X_train)
[[0.01504144 0.01329959 0.01338317 ... 0.04825363 0.0540442  0.05464265]
 [0.01329959 0.01338317 0.0088363  ... 0.0540442  0.05464265 0.05311143]
 [0.01338317 0.0088363  0.00741541 ... 0.05464265 0.05311143 0.05406761]
 ...
 [0.34368637 0.33416135 0.32935037 ... 0.47124608 0.46623115 0.47318518]
 [0.33416135 0.32935037 0.33284744 ... 0.46623115 0.47318518 0.47804966]
 [0.32935037 0.33284744 0.33276052 ... 0.47318518 0.47804966 0.47232596]]
print(X_train.shape), print(y_train.shape)
(716, 100)
(716,)
(None, None)
print(X_test.shape), print(ytest.shape)
(340, 100)
(340,)
(None, None)
# reshape input to be [samples, time steps, features] which is required for LSTM
X_train = X_train.reshape( X_train.shape[0], X_train.shape[1] , 1)
X_test = X_test.reshape( X_test.shape[0], X_test.shape[1] , 1)
# Create the Stacked LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
model = Sequential()
model.add(LSTM( 50, return_sequences = True,input_shape = (100,1) ))
model.add(LSTM( 50, return_sequences = True ))
model.add(LSTM( 50 ))
model.add(Dense( 1 ))
model.compile(loss = 'mean_squared_error',optimizer = 'adam')
model.summary()
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 (None, 100, 50)           10400     
                                                                 
 lstm_1 (LSTM)               (None, 100, 50)           20200     
                                                                 
 lstm_2 (LSTM)               (None, 50)                20200     
                                                                 
 dense (Dense)               (None, 1)                 51        
                                                                 
=================================================================
Total params: 50,851
Trainable params: 50,851
Non-trainable params: 0
_________________________________________________________________
model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=100,batch_size=64,verbose=1)
Epoch 1/100
12/12 [==============================] - 8s 242ms/step - loss: 0.0181 - val_loss: 0.1075
Epoch 2/100
12/12 [==============================] - 2s 144ms/step - loss: 0.0036 - val_loss: 0.0153
Epoch 3/100
12/12 [==============================] - 2s 138ms/step - loss: 0.0016 - val_loss: 0.0129
Epoch 4/100
12/12 [==============================] - 2s 149ms/step - loss: 9.3855e-04 - val_loss: 0.0100
Epoch 5/100
12/12 [==============================] - 2s 155ms/step - loss: 7.5708e-04 - val_loss: 0.0058
Epoch 6/100
12/12 [==============================] - 2s 178ms/step - loss: 8.1268e-04 - val_loss: 0.0034
Epoch 7/100
12/12 [==============================] - 2s 152ms/step - loss: 6.9535e-04 - val_loss: 0.0033
Epoch 8/100
12/12 [==============================] - 2s 128ms/step - loss: 6.6261e-04 - val_loss: 0.0028
Epoch 9/100
12/12 [==============================] - 1s 124ms/step - loss: 6.3809e-04 - val_loss: 0.0038
Epoch 10/100
12/12 [==============================] - 1s 125ms/step - loss: 6.3192e-04 - val_loss: 0.0045
Epoch 11/100
12/12 [==============================] - 1s 121ms/step - loss: 6.4680e-04 - val_loss: 0.0055
Epoch 12/100
12/12 [==============================] - 1s 119ms/step - loss: 6.1841e-04 - val_loss: 0.0047
Epoch 13/100
12/12 [==============================] - 1s 122ms/step - loss: 6.0439e-04 - val_loss: 0.0048
Epoch 14/100
12/12 [==============================] - 1s 121ms/step - loss: 6.1740e-04 - val_loss: 0.0043
Epoch 15/100
12/12 [==============================] - 2s 127ms/step - loss: 6.1088e-04 - val_loss: 0.0076
Epoch 16/100
12/12 [==============================] - 2s 133ms/step - loss: 6.0042e-04 - val_loss: 0.0080
Epoch 17/100
12/12 [==============================] - 2s 126ms/step - loss: 6.0921e-04 - val_loss: 0.0052
Epoch 18/100
12/12 [==============================] - 1s 121ms/step - loss: 5.7403e-04 - val_loss: 0.0095
Epoch 19/100
12/12 [==============================] - 2s 126ms/step - loss: 6.9442e-04 - val_loss: 0.0128
Epoch 20/100
12/12 [==============================] - 1s 126ms/step - loss: 6.0053e-04 - val_loss: 0.0078
Epoch 21/100
12/12 [==============================] - 2s 127ms/step - loss: 5.5342e-04 - val_loss: 0.0049
Epoch 22/100
12/12 [==============================] - 1s 126ms/step - loss: 5.3984e-04 - val_loss: 0.0098
Epoch 23/100
12/12 [==============================] - 1s 120ms/step - loss: 5.4164e-04 - val_loss: 0.0102
Epoch 24/100
12/12 [==============================] - 1s 122ms/step - loss: 5.3058e-04 - val_loss: 0.0085
Epoch 25/100
12/12 [==============================] - 2s 133ms/step - loss: 5.0888e-04 - val_loss: 0.0085
Epoch 26/100
12/12 [==============================] - 1s 122ms/step - loss: 4.9444e-04 - val_loss: 0.0118
Epoch 27/100
12/12 [==============================] - 2s 140ms/step - loss: 4.9605e-04 - val_loss: 0.0151
Epoch 28/100
12/12 [==============================] - 1s 125ms/step - loss: 4.7029e-04 - val_loss: 0.0105
Epoch 29/100
12/12 [==============================] - 2s 127ms/step - loss: 5.0042e-04 - val_loss: 0.0253
Epoch 30/100
12/12 [==============================] - 1s 124ms/step - loss: 4.7094e-04 - val_loss: 0.0261
Epoch 31/100
12/12 [==============================] - 1s 125ms/step - loss: 4.7281e-04 - val_loss: 0.0204
Epoch 32/100
12/12 [==============================] - 2s 126ms/step - loss: 4.9118e-04 - val_loss: 0.0292
Epoch 33/100
12/12 [==============================] - 1s 125ms/step - loss: 4.7579e-04 - val_loss: 0.0370
Epoch 34/100
12/12 [==============================] - 1s 125ms/step - loss: 4.4349e-04 - val_loss: 0.0282
Epoch 35/100
12/12 [==============================] - 2s 135ms/step - loss: 4.1760e-04 - val_loss: 0.0463
Epoch 36/100
12/12 [==============================] - 2s 136ms/step - loss: 4.9140e-04 - val_loss: 0.0217
Epoch 37/100
12/12 [==============================] - 2s 130ms/step - loss: 4.8531e-04 - val_loss: 0.0437
Epoch 38/100
12/12 [==============================] - 2s 125ms/step - loss: 4.4357e-04 - val_loss: 0.0254
Epoch 39/100
12/12 [==============================] - 1s 121ms/step - loss: 4.1832e-04 - val_loss: 0.0399
Epoch 40/100
12/12 [==============================] - 1s 121ms/step - loss: 4.0942e-04 - val_loss: 0.0462
Epoch 41/100
12/12 [==============================] - 1s 126ms/step - loss: 4.5296e-04 - val_loss: 0.0289
Epoch 42/100
12/12 [==============================] - 2s 131ms/step - loss: 4.1490e-04 - val_loss: 0.0335
Epoch 43/100
12/12 [==============================] - 2s 154ms/step - loss: 3.8429e-04 - val_loss: 0.0296
Epoch 44/100
12/12 [==============================] - 2s 157ms/step - loss: 3.8030e-04 - val_loss: 0.0278
Epoch 45/100
12/12 [==============================] - 2s 136ms/step - loss: 3.7881e-04 - val_loss: 0.0335
Epoch 46/100
12/12 [==============================] - 2s 133ms/step - loss: 3.8968e-04 - val_loss: 0.0288
Epoch 47/100
12/12 [==============================] - 2s 143ms/step - loss: 4.0224e-04 - val_loss: 0.0343
Epoch 48/100
12/12 [==============================] - 2s 135ms/step - loss: 4.4134e-04 - val_loss: 0.0207
Epoch 49/100
12/12 [==============================] - 2s 134ms/step - loss: 3.9634e-04 - val_loss: 0.0321
Epoch 50/100
12/12 [==============================] - 2s 130ms/step - loss: 4.1413e-04 - val_loss: 0.0187
Epoch 51/100
12/12 [==============================] - 2s 134ms/step - loss: 4.7269e-04 - val_loss: 0.0241
Epoch 52/100
12/12 [==============================] - 2s 137ms/step - loss: 3.5420e-04 - val_loss: 0.0212
Epoch 53/100
12/12 [==============================] - 2s 142ms/step - loss: 3.4521e-04 - val_loss: 0.0254
Epoch 54/100
12/12 [==============================] - 2s 141ms/step - loss: 3.6213e-04 - val_loss: 0.0229
Epoch 55/100
12/12 [==============================] - 2s 142ms/step - loss: 3.5286e-04 - val_loss: 0.0168
Epoch 56/100
12/12 [==============================] - 2s 138ms/step - loss: 3.5525e-04 - val_loss: 0.0190
Epoch 57/100
12/12 [==============================] - 2s 147ms/step - loss: 3.3352e-04 - val_loss: 0.0186
Epoch 58/100
12/12 [==============================] - 2s 136ms/step - loss: 3.3236e-04 - val_loss: 0.0154
Epoch 59/100
12/12 [==============================] - 2s 131ms/step - loss: 3.4578e-04 - val_loss: 0.0247
Epoch 60/100
12/12 [==============================] - 2s 134ms/step - loss: 3.7750e-04 - val_loss: 0.0152
Epoch 61/100
12/12 [==============================] - 2s 131ms/step - loss: 3.3259e-04 - val_loss: 0.0148
Epoch 62/100
12/12 [==============================] - 2s 131ms/step - loss: 3.4476e-04 - val_loss: 0.0232
Epoch 63/100
12/12 [==============================] - 2s 131ms/step - loss: 3.1546e-04 - val_loss: 0.0138
Epoch 64/100
12/12 [==============================] - 2s 131ms/step - loss: 3.1127e-04 - val_loss: 0.0171
Epoch 65/100
12/12 [==============================] - 2s 132ms/step - loss: 3.2984e-04 - val_loss: 0.0192
Epoch 66/100
12/12 [==============================] - 2s 132ms/step - loss: 3.3698e-04 - val_loss: 0.0101
Epoch 67/100
12/12 [==============================] - 2s 143ms/step - loss: 3.1672e-04 - val_loss: 0.0121-0
Epoch 68/100
12/12 [==============================] - 2s 134ms/step - loss: 2.8723e-04 - val_loss: 0.0154
Epoch 69/100
12/12 [==============================] - 2s 136ms/step - loss: 2.8417e-04 - val_loss: 0.0083
Epoch 70/100
12/12 [==============================] - 2s 165ms/step - loss: 3.1323e-04 - val_loss: 0.0141
Epoch 71/100
12/12 [==============================] - 2s 163ms/step - loss: 2.7872e-04 - val_loss: 0.0111
Epoch 72/100
12/12 [==============================] - 2s 142ms/step - loss: 2.8723e-04 - val_loss: 0.0139
Epoch 73/100
12/12 [==============================] - 2s 134ms/step - loss: 2.7782e-04 - val_loss: 0.0099
Epoch 74/100
12/12 [==============================] - 2s 134ms/step - loss: 2.7595e-04 - val_loss: 0.0090
Epoch 75/100
12/12 [==============================] - 2s 133ms/step - loss: 2.6279e-04 - val_loss: 0.0075
Epoch 76/100
12/12 [==============================] - 2s 134ms/step - loss: 2.6934e-04 - val_loss: 0.0095
Epoch 77/100
12/12 [==============================] - 2s 152ms/step - loss: 2.7038e-04 - val_loss: 0.0050
Epoch 78/100
12/12 [==============================] - 2s 131ms/step - loss: 3.0162e-04 - val_loss: 0.0117
Epoch 79/100
12/12 [==============================] - 2s 130ms/step - loss: 2.8454e-04 - val_loss: 0.0089
Epoch 80/100
12/12 [==============================] - 2s 132ms/step - loss: 2.5627e-04 - val_loss: 0.0043
Epoch 81/100
12/12 [==============================] - 2s 134ms/step - loss: 2.5032e-04 - val_loss: 0.0096
Epoch 82/100
12/12 [==============================] - 2s 133ms/step - loss: 2.6968e-04 - val_loss: 0.0050
Epoch 83/100
12/12 [==============================] - 2s 132ms/step - loss: 2.3485e-04 - val_loss: 0.0068
Epoch 84/100
12/12 [==============================] - 2s 131ms/step - loss: 2.3578e-04 - val_loss: 0.0017
Epoch 85/100
12/12 [==============================] - 2s 131ms/step - loss: 2.4573e-04 - val_loss: 0.0053
Epoch 86/100
12/12 [==============================] - 2s 131ms/step - loss: 2.3321e-04 - val_loss: 0.0032
Epoch 87/100
12/12 [==============================] - 2s 151ms/step - loss: 2.3794e-04 - val_loss: 0.0021
Epoch 88/100
12/12 [==============================] - 2s 138ms/step - loss: 2.2080e-04 - val_loss: 0.0026
Epoch 89/100
12/12 [==============================] - 2s 143ms/step - loss: 2.1098e-04 - val_loss: 0.0039
Epoch 90/100
12/12 [==============================] - 2s 142ms/step - loss: 2.0805e-04 - val_loss: 0.0029
Epoch 91/100
12/12 [==============================] - 2s 140ms/step - loss: 2.0541e-04 - val_loss: 0.0017
Epoch 92/100
12/12 [==============================] - 2s 148ms/step - loss: 1.9976e-04 - val_loss: 0.0026
Epoch 93/100
12/12 [==============================] - 2s 146ms/step - loss: 2.0156e-04 - val_loss: 0.0010
Epoch 94/100
12/12 [==============================] - 2s 143ms/step - loss: 1.9975e-04 - val_loss: 0.0012
Epoch 95/100
12/12 [==============================] - 2s 134ms/step - loss: 1.9878e-04 - val_loss: 0.0023
Epoch 96/100
12/12 [==============================] - 2s 150ms/step - loss: 1.8902e-04 - val_loss: 0.0011
Epoch 97/100
12/12 [==============================] - 2s 142ms/step - loss: 1.9789e-04 - val_loss: 0.0040
Epoch 98/100
12/12 [==============================] - 2s 138ms/step - loss: 1.9757e-04 - val_loss: 9.4747e-04
Epoch 99/100
12/12 [==============================] - 2s 135ms/step - loss: 1.8185e-04 - val_loss: 0.0026
Epoch 100/100
12/12 [==============================] - 2s 132ms/step - loss: 1.8153e-04 - val_loss: 8.8029e-04
<keras.callbacks.History at 0x20de05b2730>
import tensorflow as tf
tf.__version__
'2.7.0'
# Lets Do the prediction and check performance metrics
train_predict = model.predict(X_train)
test_predict = model.predict(X_test)
##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
# Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train, train_predict))
1595.787429164462
# Test Data RMSE
math.sqrt(mean_squared_error(ytest, test_predict))
3253.1130171758036
import numpy as np
# shift train predictions for plotting
look_back = 100
trainPredictPlot = np.empty_like(df1)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
# shift test predictions for plotting
testPredictPlot = np.empty_like(df1)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict
# plot baseline and predictions
plt.plot(scaler.inverse_transform(df1))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
plt.show()

len(test_data)
441
x_input=test_data[341:].reshape(1,-1)
x_input.shape
(1, 100)
temp_input=list(x_input)
temp_input=temp_input[0].tolist()
temp_input
[0.9002229971214315,
 0.9263039648018936,
 0.9813879314091611,
 0.9883553377219523,
 1.0,
 0.995964654789089,
 0.9957005352599573,
 0.9819295436081403,
 0.9833738428054175,
 0.9664969392224187,
 0.9472496464475924,
 0.939212388877559,
 0.9471025418997217,
 0.9511178273995593,
 0.9687804029995957,
 0.9750022567174956,
 0.9894385621199104,
 0.9648888190513762,
 0.9662027301266771,
 0.9560391431828743,
 0.864991457906368,
 0.8662919958409532,
 0.8779132551227486,
 0.8740617905966761,
 0.881172958172159,
 0.8707920576917292,
 0.8697656691418121,
 0.8626812478477601,
 0.853129482091693,
 0.85693748391044,
 0.8537513331349651,
 0.8554296622947641,
 0.8363629069195975,
 0.8227423631008302,
 0.8182389579648754,
 0.822317765883112,
 0.8443567017823053,
 0.8576997529312254,
 0.8554931847131628,
 0.8611165903840432,
 0.872360058440625,
 0.8964116520175056,
 0.9128673016679649,
 0.9156121387998275,
 0.9103030019357621,
 0.9152945267078338,
 0.9257389496066626,
 0.931158414881631,
 0.9173372739521308,
 0.9123190028986283,
 0.908313747254327,
 0.9059166117810684,
 0.9145389442574063,
 0.9187013343051149,
 0.9101024048250292,
 0.8743994623997433,
 0.8703540873332956,
 0.8825303319547855,
 0.8945494421728679,
 0.8977322496631641,
 0.8911392912904079,
 0.8611032172433277,
 0.8561417820378661,
 0.850765779470223,
 0.8501706747083821,
 0.8189176448561888,
 0.8293553811846597,
 0.8430661937032569,
 0.8565797523962997,
 0.8519626755642629,
 0.8378138926872325,
 0.8381582510606573,
 0.8505116897966281,
 0.8557205281053268,
 0.892215829118008,
 0.9048267008127526,
 0.9039607899514222,
 0.894235173366053,
 0.9009050272979235,
 0.8676527129087586,
 0.8625776060072148,
 0.8811997044535902,
 0.8866893787173151,
 0.9047698649647118,
 0.879982748648477,
 0.8618220235567875,
 0.8600300227009063,
 0.883850929600444,
 0.9149434817640509,
 0.9289819362301786,
 0.9189487374083523,
 0.9481189005941018,
 0.9166318407793868,
 0.913439003433554,
 0.9310413999003702,
 0.9379051643726157,
 0.9362402083535324,
 0.9390151350520048,
 0.9881814868926504,
 0.9816654240790086]
# demonstrate prediction for next 10 days
from numpy import array

lst_output=[]
n_steps=100
i=0
while(i<30):
    
    if(len(temp_input)>100):
        #print(temp_input)
        x_input=np.array(temp_input[1:])
        print("{} day input {}".format(i,x_input))
        x_input=x_input.reshape(1,-1)
        x_input = x_input.reshape((1, n_steps, 1))
        #print(x_input)
        yhat = model.predict(x_input, verbose=0)
        print("{} day output {}".format(i,yhat))
        temp_input.extend(yhat[0].tolist())
        temp_input=temp_input[1:]
        #print(temp_input)
        lst_output.extend(yhat.tolist())
        i=i+1
    else:
        x_input = x_input.reshape((1, n_steps,1))
        yhat = model.predict(x_input, verbose=0)
        print(yhat[0])
        temp_input.extend(yhat[0].tolist())
        print(len(temp_input))
        lst_output.extend(yhat.tolist())
        i=i+1
    

print(lst_output)
[0.95937586]
101
1 day input [0.92630396 0.98138793 0.98835534 1.         0.99596465 0.99570054
 0.98192954 0.98337384 0.96649694 0.94724965 0.93921239 0.94710254
 0.95111783 0.9687804  0.97500226 0.98943856 0.96488882 0.96620273
 0.95603914 0.86499146 0.866292   0.87791326 0.87406179 0.88117296
 0.87079206 0.86976567 0.86268125 0.85312948 0.85693748 0.85375133
 0.85542966 0.83636291 0.82274236 0.81823896 0.82231777 0.8443567
 0.85769975 0.85549318 0.86111659 0.87236006 0.89641165 0.9128673
 0.91561214 0.910303   0.91529453 0.92573895 0.93115841 0.91733727
 0.912319   0.90831375 0.90591661 0.91453894 0.91870133 0.9101024
 0.87439946 0.87035409 0.88253033 0.89454944 0.89773225 0.89113929
 0.86110322 0.85614178 0.85076578 0.85017067 0.81891764 0.82935538
 0.84306619 0.85657975 0.85196268 0.83781389 0.83815825 0.85051169
 0.85572053 0.89221583 0.9048267  0.90396079 0.89423517 0.90090503
 0.86765271 0.86257761 0.8811997  0.88668938 0.90476986 0.87998275
 0.86182202 0.86003002 0.88385093 0.91494348 0.92898194 0.91894874
 0.9481189  0.91663184 0.913439   0.9310414  0.93790516 0.93624021
 0.93901514 0.98818149 0.98166542 0.95937586]
1 day output [[0.969283]]
2 day input [0.98138793 0.98835534 1.         0.99596465 0.99570054 0.98192954
 0.98337384 0.96649694 0.94724965 0.93921239 0.94710254 0.95111783
 0.9687804  0.97500226 0.98943856 0.96488882 0.96620273 0.95603914
 0.86499146 0.866292   0.87791326 0.87406179 0.88117296 0.87079206
 0.86976567 0.86268125 0.85312948 0.85693748 0.85375133 0.85542966
 0.83636291 0.82274236 0.81823896 0.82231777 0.8443567  0.85769975
 0.85549318 0.86111659 0.87236006 0.89641165 0.9128673  0.91561214
 0.910303   0.91529453 0.92573895 0.93115841 0.91733727 0.912319
 0.90831375 0.90591661 0.91453894 0.91870133 0.9101024  0.87439946
 0.87035409 0.88253033 0.89454944 0.89773225 0.89113929 0.86110322
 0.85614178 0.85076578 0.85017067 0.81891764 0.82935538 0.84306619
 0.85657975 0.85196268 0.83781389 0.83815825 0.85051169 0.85572053
 0.89221583 0.9048267  0.90396079 0.89423517 0.90090503 0.86765271
 0.86257761 0.8811997  0.88668938 0.90476986 0.87998275 0.86182202
 0.86003002 0.88385093 0.91494348 0.92898194 0.91894874 0.9481189
 0.91663184 0.913439   0.9310414  0.93790516 0.93624021 0.93901514
 0.98818149 0.98166542 0.95937586 0.96928298]
2 day output [[0.9715092]]
3 day input [0.98835534 1.         0.99596465 0.99570054 0.98192954 0.98337384
 0.96649694 0.94724965 0.93921239 0.94710254 0.95111783 0.9687804
 0.97500226 0.98943856 0.96488882 0.96620273 0.95603914 0.86499146
 0.866292   0.87791326 0.87406179 0.88117296 0.87079206 0.86976567
 0.86268125 0.85312948 0.85693748 0.85375133 0.85542966 0.83636291
 0.82274236 0.81823896 0.82231777 0.8443567  0.85769975 0.85549318
 0.86111659 0.87236006 0.89641165 0.9128673  0.91561214 0.910303
 0.91529453 0.92573895 0.93115841 0.91733727 0.912319   0.90831375
 0.90591661 0.91453894 0.91870133 0.9101024  0.87439946 0.87035409
 0.88253033 0.89454944 0.89773225 0.89113929 0.86110322 0.85614178
 0.85076578 0.85017067 0.81891764 0.82935538 0.84306619 0.85657975
 0.85196268 0.83781389 0.83815825 0.85051169 0.85572053 0.89221583
 0.9048267  0.90396079 0.89423517 0.90090503 0.86765271 0.86257761
 0.8811997  0.88668938 0.90476986 0.87998275 0.86182202 0.86003002
 0.88385093 0.91494348 0.92898194 0.91894874 0.9481189  0.91663184
 0.913439   0.9310414  0.93790516 0.93624021 0.93901514 0.98818149
 0.98166542 0.95937586 0.96928298 0.97150922]
3 day output [[0.96960205]]
4 day input [1.         0.99596465 0.99570054 0.98192954 0.98337384 0.96649694
 0.94724965 0.93921239 0.94710254 0.95111783 0.9687804  0.97500226
 0.98943856 0.96488882 0.96620273 0.95603914 0.86499146 0.866292
 0.87791326 0.87406179 0.88117296 0.87079206 0.86976567 0.86268125
 0.85312948 0.85693748 0.85375133 0.85542966 0.83636291 0.82274236
 0.81823896 0.82231777 0.8443567  0.85769975 0.85549318 0.86111659
 0.87236006 0.89641165 0.9128673  0.91561214 0.910303   0.91529453
 0.92573895 0.93115841 0.91733727 0.912319   0.90831375 0.90591661
 0.91453894 0.91870133 0.9101024  0.87439946 0.87035409 0.88253033
 0.89454944 0.89773225 0.89113929 0.86110322 0.85614178 0.85076578
 0.85017067 0.81891764 0.82935538 0.84306619 0.85657975 0.85196268
 0.83781389 0.83815825 0.85051169 0.85572053 0.89221583 0.9048267
 0.90396079 0.89423517 0.90090503 0.86765271 0.86257761 0.8811997
 0.88668938 0.90476986 0.87998275 0.86182202 0.86003002 0.88385093
 0.91494348 0.92898194 0.91894874 0.9481189  0.91663184 0.913439
 0.9310414  0.93790516 0.93624021 0.93901514 0.98818149 0.98166542
 0.95937586 0.96928298 0.97150922 0.96960205]
4 day output [[0.965979]]
5 day input [0.99596465 0.99570054 0.98192954 0.98337384 0.96649694 0.94724965
 0.93921239 0.94710254 0.95111783 0.9687804  0.97500226 0.98943856
 0.96488882 0.96620273 0.95603914 0.86499146 0.866292   0.87791326
 0.87406179 0.88117296 0.87079206 0.86976567 0.86268125 0.85312948
 0.85693748 0.85375133 0.85542966 0.83636291 0.82274236 0.81823896
 0.82231777 0.8443567  0.85769975 0.85549318 0.86111659 0.87236006
 0.89641165 0.9128673  0.91561214 0.910303   0.91529453 0.92573895
 0.93115841 0.91733727 0.912319   0.90831375 0.90591661 0.91453894
 0.91870133 0.9101024  0.87439946 0.87035409 0.88253033 0.89454944
 0.89773225 0.89113929 0.86110322 0.85614178 0.85076578 0.85017067
 0.81891764 0.82935538 0.84306619 0.85657975 0.85196268 0.83781389
 0.83815825 0.85051169 0.85572053 0.89221583 0.9048267  0.90396079
 0.89423517 0.90090503 0.86765271 0.86257761 0.8811997  0.88668938
 0.90476986 0.87998275 0.86182202 0.86003002 0.88385093 0.91494348
 0.92898194 0.91894874 0.9481189  0.91663184 0.913439   0.9310414
 0.93790516 0.93624021 0.93901514 0.98818149 0.98166542 0.95937586
 0.96928298 0.97150922 0.96960205 0.96597898]
5 day output [[0.9618533]]
6 day input [0.99570054 0.98192954 0.98337384 0.96649694 0.94724965 0.93921239
 0.94710254 0.95111783 0.9687804  0.97500226 0.98943856 0.96488882
 0.96620273 0.95603914 0.86499146 0.866292   0.87791326 0.87406179
 0.88117296 0.87079206 0.86976567 0.86268125 0.85312948 0.85693748
 0.85375133 0.85542966 0.83636291 0.82274236 0.81823896 0.82231777
 0.8443567  0.85769975 0.85549318 0.86111659 0.87236006 0.89641165
 0.9128673  0.91561214 0.910303   0.91529453 0.92573895 0.93115841
 0.91733727 0.912319   0.90831375 0.90591661 0.91453894 0.91870133
 0.9101024  0.87439946 0.87035409 0.88253033 0.89454944 0.89773225
 0.89113929 0.86110322 0.85614178 0.85076578 0.85017067 0.81891764
 0.82935538 0.84306619 0.85657975 0.85196268 0.83781389 0.83815825
 0.85051169 0.85572053 0.89221583 0.9048267  0.90396079 0.89423517
 0.90090503 0.86765271 0.86257761 0.8811997  0.88668938 0.90476986
 0.87998275 0.86182202 0.86003002 0.88385093 0.91494348 0.92898194
 0.91894874 0.9481189  0.91663184 0.913439   0.9310414  0.93790516
 0.93624021 0.93901514 0.98818149 0.98166542 0.95937586 0.96928298
 0.97150922 0.96960205 0.96597898 0.96185333]
6 day output [[0.95775884]]
7 day input [0.98192954 0.98337384 0.96649694 0.94724965 0.93921239 0.94710254
 0.95111783 0.9687804  0.97500226 0.98943856 0.96488882 0.96620273
 0.95603914 0.86499146 0.866292   0.87791326 0.87406179 0.88117296
 0.87079206 0.86976567 0.86268125 0.85312948 0.85693748 0.85375133
 0.85542966 0.83636291 0.82274236 0.81823896 0.82231777 0.8443567
 0.85769975 0.85549318 0.86111659 0.87236006 0.89641165 0.9128673
 0.91561214 0.910303   0.91529453 0.92573895 0.93115841 0.91733727
 0.912319   0.90831375 0.90591661 0.91453894 0.91870133 0.9101024
 0.87439946 0.87035409 0.88253033 0.89454944 0.89773225 0.89113929
 0.86110322 0.85614178 0.85076578 0.85017067 0.81891764 0.82935538
 0.84306619 0.85657975 0.85196268 0.83781389 0.83815825 0.85051169
 0.85572053 0.89221583 0.9048267  0.90396079 0.89423517 0.90090503
 0.86765271 0.86257761 0.8811997  0.88668938 0.90476986 0.87998275
 0.86182202 0.86003002 0.88385093 0.91494348 0.92898194 0.91894874
 0.9481189  0.91663184 0.913439   0.9310414  0.93790516 0.93624021
 0.93901514 0.98818149 0.98166542 0.95937586 0.96928298 0.97150922
 0.96960205 0.96597898 0.96185333 0.95775884]
7 day output [[0.9539533]]
8 day input [0.98337384 0.96649694 0.94724965 0.93921239 0.94710254 0.95111783
 0.9687804  0.97500226 0.98943856 0.96488882 0.96620273 0.95603914
 0.86499146 0.866292   0.87791326 0.87406179 0.88117296 0.87079206
 0.86976567 0.86268125 0.85312948 0.85693748 0.85375133 0.85542966
 0.83636291 0.82274236 0.81823896 0.82231777 0.8443567  0.85769975
 0.85549318 0.86111659 0.87236006 0.89641165 0.9128673  0.91561214
 0.910303   0.91529453 0.92573895 0.93115841 0.91733727 0.912319
 0.90831375 0.90591661 0.91453894 0.91870133 0.9101024  0.87439946
 0.87035409 0.88253033 0.89454944 0.89773225 0.89113929 0.86110322
 0.85614178 0.85076578 0.85017067 0.81891764 0.82935538 0.84306619
 0.85657975 0.85196268 0.83781389 0.83815825 0.85051169 0.85572053
 0.89221583 0.9048267  0.90396079 0.89423517 0.90090503 0.86765271
 0.86257761 0.8811997  0.88668938 0.90476986 0.87998275 0.86182202
 0.86003002 0.88385093 0.91494348 0.92898194 0.91894874 0.9481189
 0.91663184 0.913439   0.9310414  0.93790516 0.93624021 0.93901514
 0.98818149 0.98166542 0.95937586 0.96928298 0.97150922 0.96960205
 0.96597898 0.96185333 0.95775884 0.95395333]
8 day output [[0.9506052]]
9 day input [0.96649694 0.94724965 0.93921239 0.94710254 0.95111783 0.9687804
 0.97500226 0.98943856 0.96488882 0.96620273 0.95603914 0.86499146
 0.866292   0.87791326 0.87406179 0.88117296 0.87079206 0.86976567
 0.86268125 0.85312948 0.85693748 0.85375133 0.85542966 0.83636291
 0.82274236 0.81823896 0.82231777 0.8443567  0.85769975 0.85549318
 0.86111659 0.87236006 0.89641165 0.9128673  0.91561214 0.910303
 0.91529453 0.92573895 0.93115841 0.91733727 0.912319   0.90831375
 0.90591661 0.91453894 0.91870133 0.9101024  0.87439946 0.87035409
 0.88253033 0.89454944 0.89773225 0.89113929 0.86110322 0.85614178
 0.85076578 0.85017067 0.81891764 0.82935538 0.84306619 0.85657975
 0.85196268 0.83781389 0.83815825 0.85051169 0.85572053 0.89221583
 0.9048267  0.90396079 0.89423517 0.90090503 0.86765271 0.86257761
 0.8811997  0.88668938 0.90476986 0.87998275 0.86182202 0.86003002
 0.88385093 0.91494348 0.92898194 0.91894874 0.9481189  0.91663184
 0.913439   0.9310414  0.93790516 0.93624021 0.93901514 0.98818149
 0.98166542 0.95937586 0.96928298 0.97150922 0.96960205 0.96597898
 0.96185333 0.95775884 0.95395333 0.95060521]
9 day output [[0.9478434]]
10 day input [0.94724965 0.93921239 0.94710254 0.95111783 0.9687804  0.97500226
 0.98943856 0.96488882 0.96620273 0.95603914 0.86499146 0.866292
 0.87791326 0.87406179 0.88117296 0.87079206 0.86976567 0.86268125
 0.85312948 0.85693748 0.85375133 0.85542966 0.83636291 0.82274236
 0.81823896 0.82231777 0.8443567  0.85769975 0.85549318 0.86111659
 0.87236006 0.89641165 0.9128673  0.91561214 0.910303   0.91529453
 0.92573895 0.93115841 0.91733727 0.912319   0.90831375 0.90591661
 0.91453894 0.91870133 0.9101024  0.87439946 0.87035409 0.88253033
 0.89454944 0.89773225 0.89113929 0.86110322 0.85614178 0.85076578
 0.85017067 0.81891764 0.82935538 0.84306619 0.85657975 0.85196268
 0.83781389 0.83815825 0.85051169 0.85572053 0.89221583 0.9048267
 0.90396079 0.89423517 0.90090503 0.86765271 0.86257761 0.8811997
 0.88668938 0.90476986 0.87998275 0.86182202 0.86003002 0.88385093
 0.91494348 0.92898194 0.91894874 0.9481189  0.91663184 0.913439
 0.9310414  0.93790516 0.93624021 0.93901514 0.98818149 0.98166542
 0.95937586 0.96928298 0.97150922 0.96960205 0.96597898 0.96185333
 0.95775884 0.95395333 0.95060521 0.94784337]
10 day output [[0.94575524]]
11 day input [0.93921239 0.94710254 0.95111783 0.9687804  0.97500226 0.98943856
 0.96488882 0.96620273 0.95603914 0.86499146 0.866292   0.87791326
 0.87406179 0.88117296 0.87079206 0.86976567 0.86268125 0.85312948
 0.85693748 0.85375133 0.85542966 0.83636291 0.82274236 0.81823896
 0.82231777 0.8443567  0.85769975 0.85549318 0.86111659 0.87236006
 0.89641165 0.9128673  0.91561214 0.910303   0.91529453 0.92573895
 0.93115841 0.91733727 0.912319   0.90831375 0.90591661 0.91453894
 0.91870133 0.9101024  0.87439946 0.87035409 0.88253033 0.89454944
 0.89773225 0.89113929 0.86110322 0.85614178 0.85076578 0.85017067
 0.81891764 0.82935538 0.84306619 0.85657975 0.85196268 0.83781389
 0.83815825 0.85051169 0.85572053 0.89221583 0.9048267  0.90396079
 0.89423517 0.90090503 0.86765271 0.86257761 0.8811997  0.88668938
 0.90476986 0.87998275 0.86182202 0.86003002 0.88385093 0.91494348
 0.92898194 0.91894874 0.9481189  0.91663184 0.913439   0.9310414
 0.93790516 0.93624021 0.93901514 0.98818149 0.98166542 0.95937586
 0.96928298 0.97150922 0.96960205 0.96597898 0.96185333 0.95775884
 0.95395333 0.95060521 0.94784337 0.94575524]
11 day output [[0.94437236]]
12 day input [0.94710254 0.95111783 0.9687804  0.97500226 0.98943856 0.96488882
 0.96620273 0.95603914 0.86499146 0.866292   0.87791326 0.87406179
 0.88117296 0.87079206 0.86976567 0.86268125 0.85312948 0.85693748
 0.85375133 0.85542966 0.83636291 0.82274236 0.81823896 0.82231777
 0.8443567  0.85769975 0.85549318 0.86111659 0.87236006 0.89641165
 0.9128673  0.91561214 0.910303   0.91529453 0.92573895 0.93115841
 0.91733727 0.912319   0.90831375 0.90591661 0.91453894 0.91870133
 0.9101024  0.87439946 0.87035409 0.88253033 0.89454944 0.89773225
 0.89113929 0.86110322 0.85614178 0.85076578 0.85017067 0.81891764
 0.82935538 0.84306619 0.85657975 0.85196268 0.83781389 0.83815825
 0.85051169 0.85572053 0.89221583 0.9048267  0.90396079 0.89423517
 0.90090503 0.86765271 0.86257761 0.8811997  0.88668938 0.90476986
 0.87998275 0.86182202 0.86003002 0.88385093 0.91494348 0.92898194
 0.91894874 0.9481189  0.91663184 0.913439   0.9310414  0.93790516
 0.93624021 0.93901514 0.98818149 0.98166542 0.95937586 0.96928298
 0.97150922 0.96960205 0.96597898 0.96185333 0.95775884 0.95395333
 0.95060521 0.94784337 0.94575524 0.94437236]
12 day output [[0.9436725]]
13 day input [0.95111783 0.9687804  0.97500226 0.98943856 0.96488882 0.96620273
 0.95603914 0.86499146 0.866292   0.87791326 0.87406179 0.88117296
 0.87079206 0.86976567 0.86268125 0.85312948 0.85693748 0.85375133
 0.85542966 0.83636291 0.82274236 0.81823896 0.82231777 0.8443567
 0.85769975 0.85549318 0.86111659 0.87236006 0.89641165 0.9128673
 0.91561214 0.910303   0.91529453 0.92573895 0.93115841 0.91733727
 0.912319   0.90831375 0.90591661 0.91453894 0.91870133 0.9101024
 0.87439946 0.87035409 0.88253033 0.89454944 0.89773225 0.89113929
 0.86110322 0.85614178 0.85076578 0.85017067 0.81891764 0.82935538
 0.84306619 0.85657975 0.85196268 0.83781389 0.83815825 0.85051169
 0.85572053 0.89221583 0.9048267  0.90396079 0.89423517 0.90090503
 0.86765271 0.86257761 0.8811997  0.88668938 0.90476986 0.87998275
 0.86182202 0.86003002 0.88385093 0.91494348 0.92898194 0.91894874
 0.9481189  0.91663184 0.913439   0.9310414  0.93790516 0.93624021
 0.93901514 0.98818149 0.98166542 0.95937586 0.96928298 0.97150922
 0.96960205 0.96597898 0.96185333 0.95775884 0.95395333 0.95060521
 0.94784337 0.94575524 0.94437236 0.94367248]
13 day output [[0.9435821]]
14 day input [0.9687804  0.97500226 0.98943856 0.96488882 0.96620273 0.95603914
 0.86499146 0.866292   0.87791326 0.87406179 0.88117296 0.87079206
 0.86976567 0.86268125 0.85312948 0.85693748 0.85375133 0.85542966
 0.83636291 0.82274236 0.81823896 0.82231777 0.8443567  0.85769975
 0.85549318 0.86111659 0.87236006 0.89641165 0.9128673  0.91561214
 0.910303   0.91529453 0.92573895 0.93115841 0.91733727 0.912319
 0.90831375 0.90591661 0.91453894 0.91870133 0.9101024  0.87439946
 0.87035409 0.88253033 0.89454944 0.89773225 0.89113929 0.86110322
 0.85614178 0.85076578 0.85017067 0.81891764 0.82935538 0.84306619
 0.85657975 0.85196268 0.83781389 0.83815825 0.85051169 0.85572053
 0.89221583 0.9048267  0.90396079 0.89423517 0.90090503 0.86765271
 0.86257761 0.8811997  0.88668938 0.90476986 0.87998275 0.86182202
 0.86003002 0.88385093 0.91494348 0.92898194 0.91894874 0.9481189
 0.91663184 0.913439   0.9310414  0.93790516 0.93624021 0.93901514
 0.98818149 0.98166542 0.95937586 0.96928298 0.97150922 0.96960205
 0.96597898 0.96185333 0.95775884 0.95395333 0.95060521 0.94784337
 0.94575524 0.94437236 0.94367248 0.94358212]
14 day output [[0.94399136]]
15 day input [0.97500226 0.98943856 0.96488882 0.96620273 0.95603914 0.86499146
 0.866292   0.87791326 0.87406179 0.88117296 0.87079206 0.86976567
 0.86268125 0.85312948 0.85693748 0.85375133 0.85542966 0.83636291
 0.82274236 0.81823896 0.82231777 0.8443567  0.85769975 0.85549318
 0.86111659 0.87236006 0.89641165 0.9128673  0.91561214 0.910303
 0.91529453 0.92573895 0.93115841 0.91733727 0.912319   0.90831375
 0.90591661 0.91453894 0.91870133 0.9101024  0.87439946 0.87035409
 0.88253033 0.89454944 0.89773225 0.89113929 0.86110322 0.85614178
 0.85076578 0.85017067 0.81891764 0.82935538 0.84306619 0.85657975
 0.85196268 0.83781389 0.83815825 0.85051169 0.85572053 0.89221583
 0.9048267  0.90396079 0.89423517 0.90090503 0.86765271 0.86257761
 0.8811997  0.88668938 0.90476986 0.87998275 0.86182202 0.86003002
 0.88385093 0.91494348 0.92898194 0.91894874 0.9481189  0.91663184
 0.913439   0.9310414  0.93790516 0.93624021 0.93901514 0.98818149
 0.98166542 0.95937586 0.96928298 0.97150922 0.96960205 0.96597898
 0.96185333 0.95775884 0.95395333 0.95060521 0.94784337 0.94575524
 0.94437236 0.94367248 0.94358212 0.94399136]
15 day output [[0.9447659]]
16 day input [0.98943856 0.96488882 0.96620273 0.95603914 0.86499146 0.866292
 0.87791326 0.87406179 0.88117296 0.87079206 0.86976567 0.86268125
 0.85312948 0.85693748 0.85375133 0.85542966 0.83636291 0.82274236
 0.81823896 0.82231777 0.8443567  0.85769975 0.85549318 0.86111659
 0.87236006 0.89641165 0.9128673  0.91561214 0.910303   0.91529453
 0.92573895 0.93115841 0.91733727 0.912319   0.90831375 0.90591661
 0.91453894 0.91870133 0.9101024  0.87439946 0.87035409 0.88253033
 0.89454944 0.89773225 0.89113929 0.86110322 0.85614178 0.85076578
 0.85017067 0.81891764 0.82935538 0.84306619 0.85657975 0.85196268
 0.83781389 0.83815825 0.85051169 0.85572053 0.89221583 0.9048267
 0.90396079 0.89423517 0.90090503 0.86765271 0.86257761 0.8811997
 0.88668938 0.90476986 0.87998275 0.86182202 0.86003002 0.88385093
 0.91494348 0.92898194 0.91894874 0.9481189  0.91663184 0.913439
 0.9310414  0.93790516 0.93624021 0.93901514 0.98818149 0.98166542
 0.95937586 0.96928298 0.97150922 0.96960205 0.96597898 0.96185333
 0.95775884 0.95395333 0.95060521 0.94784337 0.94575524 0.94437236
 0.94367248 0.94358212 0.94399136 0.94476593]
16 day output [[0.9457619]]
17 day input [0.96488882 0.96620273 0.95603914 0.86499146 0.866292   0.87791326
 0.87406179 0.88117296 0.87079206 0.86976567 0.86268125 0.85312948
 0.85693748 0.85375133 0.85542966 0.83636291 0.82274236 0.81823896
 0.82231777 0.8443567  0.85769975 0.85549318 0.86111659 0.87236006
 0.89641165 0.9128673  0.91561214 0.910303   0.91529453 0.92573895
 0.93115841 0.91733727 0.912319   0.90831375 0.90591661 0.91453894
 0.91870133 0.9101024  0.87439946 0.87035409 0.88253033 0.89454944
 0.89773225 0.89113929 0.86110322 0.85614178 0.85076578 0.85017067
 0.81891764 0.82935538 0.84306619 0.85657975 0.85196268 0.83781389
 0.83815825 0.85051169 0.85572053 0.89221583 0.9048267  0.90396079
 0.89423517 0.90090503 0.86765271 0.86257761 0.8811997  0.88668938
 0.90476986 0.87998275 0.86182202 0.86003002 0.88385093 0.91494348
 0.92898194 0.91894874 0.9481189  0.91663184 0.913439   0.9310414
 0.93790516 0.93624021 0.93901514 0.98818149 0.98166542 0.95937586
 0.96928298 0.97150922 0.96960205 0.96597898 0.96185333 0.95775884
 0.95395333 0.95060521 0.94784337 0.94575524 0.94437236 0.94367248
 0.94358212 0.94399136 0.94476593 0.94576192]
17 day output [[0.9468383]]
18 day input [0.96620273 0.95603914 0.86499146 0.866292   0.87791326 0.87406179
 0.88117296 0.87079206 0.86976567 0.86268125 0.85312948 0.85693748
 0.85375133 0.85542966 0.83636291 0.82274236 0.81823896 0.82231777
 0.8443567  0.85769975 0.85549318 0.86111659 0.87236006 0.89641165
 0.9128673  0.91561214 0.910303   0.91529453 0.92573895 0.93115841
 0.91733727 0.912319   0.90831375 0.90591661 0.91453894 0.91870133
 0.9101024  0.87439946 0.87035409 0.88253033 0.89454944 0.89773225
 0.89113929 0.86110322 0.85614178 0.85076578 0.85017067 0.81891764
 0.82935538 0.84306619 0.85657975 0.85196268 0.83781389 0.83815825
 0.85051169 0.85572053 0.89221583 0.9048267  0.90396079 0.89423517
 0.90090503 0.86765271 0.86257761 0.8811997  0.88668938 0.90476986
 0.87998275 0.86182202 0.86003002 0.88385093 0.91494348 0.92898194
 0.91894874 0.9481189  0.91663184 0.913439   0.9310414  0.93790516
 0.93624021 0.93901514 0.98818149 0.98166542 0.95937586 0.96928298
 0.97150922 0.96960205 0.96597898 0.96185333 0.95775884 0.95395333
 0.95060521 0.94784337 0.94575524 0.94437236 0.94367248 0.94358212
 0.94399136 0.94476593 0.94576192 0.94683832]
18 day output [[0.9478669]]
19 day input [0.95603914 0.86499146 0.866292   0.87791326 0.87406179 0.88117296
 0.87079206 0.86976567 0.86268125 0.85312948 0.85693748 0.85375133
 0.85542966 0.83636291 0.82274236 0.81823896 0.82231777 0.8443567
 0.85769975 0.85549318 0.86111659 0.87236006 0.89641165 0.9128673
 0.91561214 0.910303   0.91529453 0.92573895 0.93115841 0.91733727
 0.912319   0.90831375 0.90591661 0.91453894 0.91870133 0.9101024
 0.87439946 0.87035409 0.88253033 0.89454944 0.89773225 0.89113929
 0.86110322 0.85614178 0.85076578 0.85017067 0.81891764 0.82935538
 0.84306619 0.85657975 0.85196268 0.83781389 0.83815825 0.85051169
 0.85572053 0.89221583 0.9048267  0.90396079 0.89423517 0.90090503
 0.86765271 0.86257761 0.8811997  0.88668938 0.90476986 0.87998275
 0.86182202 0.86003002 0.88385093 0.91494348 0.92898194 0.91894874
 0.9481189  0.91663184 0.913439   0.9310414  0.93790516 0.93624021
 0.93901514 0.98818149 0.98166542 0.95937586 0.96928298 0.97150922
 0.96960205 0.96597898 0.96185333 0.95775884 0.95395333 0.95060521
 0.94784337 0.94575524 0.94437236 0.94367248 0.94358212 0.94399136
 0.94476593 0.94576192 0.94683832 0.94786692]
19 day output [[0.94873846]]
20 day input [0.86499146 0.866292   0.87791326 0.87406179 0.88117296 0.87079206
 0.86976567 0.86268125 0.85312948 0.85693748 0.85375133 0.85542966
 0.83636291 0.82274236 0.81823896 0.82231777 0.8443567  0.85769975
 0.85549318 0.86111659 0.87236006 0.89641165 0.9128673  0.91561214
 0.910303   0.91529453 0.92573895 0.93115841 0.91733727 0.912319
 0.90831375 0.90591661 0.91453894 0.91870133 0.9101024  0.87439946
 0.87035409 0.88253033 0.89454944 0.89773225 0.89113929 0.86110322
 0.85614178 0.85076578 0.85017067 0.81891764 0.82935538 0.84306619
 0.85657975 0.85196268 0.83781389 0.83815825 0.85051169 0.85572053
 0.89221583 0.9048267  0.90396079 0.89423517 0.90090503 0.86765271
 0.86257761 0.8811997  0.88668938 0.90476986 0.87998275 0.86182202
 0.86003002 0.88385093 0.91494348 0.92898194 0.91894874 0.9481189
 0.91663184 0.913439   0.9310414  0.93790516 0.93624021 0.93901514
 0.98818149 0.98166542 0.95937586 0.96928298 0.97150922 0.96960205
 0.96597898 0.96185333 0.95775884 0.95395333 0.95060521 0.94784337
 0.94575524 0.94437236 0.94367248 0.94358212 0.94399136 0.94476593
 0.94576192 0.94683832 0.94786692 0.94873846]
20 day output [[0.94937044]]
21 day input [0.866292   0.87791326 0.87406179 0.88117296 0.87079206 0.86976567
 0.86268125 0.85312948 0.85693748 0.85375133 0.85542966 0.83636291
 0.82274236 0.81823896 0.82231777 0.8443567  0.85769975 0.85549318
 0.86111659 0.87236006 0.89641165 0.9128673  0.91561214 0.910303
 0.91529453 0.92573895 0.93115841 0.91733727 0.912319   0.90831375
 0.90591661 0.91453894 0.91870133 0.9101024  0.87439946 0.87035409
 0.88253033 0.89454944 0.89773225 0.89113929 0.86110322 0.85614178
 0.85076578 0.85017067 0.81891764 0.82935538 0.84306619 0.85657975
 0.85196268 0.83781389 0.83815825 0.85051169 0.85572053 0.89221583
 0.9048267  0.90396079 0.89423517 0.90090503 0.86765271 0.86257761
 0.8811997  0.88668938 0.90476986 0.87998275 0.86182202 0.86003002
 0.88385093 0.91494348 0.92898194 0.91894874 0.9481189  0.91663184
 0.913439   0.9310414  0.93790516 0.93624021 0.93901514 0.98818149
 0.98166542 0.95937586 0.96928298 0.97150922 0.96960205 0.96597898
 0.96185333 0.95775884 0.95395333 0.95060521 0.94784337 0.94575524
 0.94437236 0.94367248 0.94358212 0.94399136 0.94476593 0.94576192
 0.94683832 0.94786692 0.94873846 0.94937044]
21 day output [[0.9497069]]
22 day input [0.87791326 0.87406179 0.88117296 0.87079206 0.86976567 0.86268125
 0.85312948 0.85693748 0.85375133 0.85542966 0.83636291 0.82274236
 0.81823896 0.82231777 0.8443567  0.85769975 0.85549318 0.86111659
 0.87236006 0.89641165 0.9128673  0.91561214 0.910303   0.91529453
 0.92573895 0.93115841 0.91733727 0.912319   0.90831375 0.90591661
 0.91453894 0.91870133 0.9101024  0.87439946 0.87035409 0.88253033
 0.89454944 0.89773225 0.89113929 0.86110322 0.85614178 0.85076578
 0.85017067 0.81891764 0.82935538 0.84306619 0.85657975 0.85196268
 0.83781389 0.83815825 0.85051169 0.85572053 0.89221583 0.9048267
 0.90396079 0.89423517 0.90090503 0.86765271 0.86257761 0.8811997
 0.88668938 0.90476986 0.87998275 0.86182202 0.86003002 0.88385093
 0.91494348 0.92898194 0.91894874 0.9481189  0.91663184 0.913439
 0.9310414  0.93790516 0.93624021 0.93901514 0.98818149 0.98166542
 0.95937586 0.96928298 0.97150922 0.96960205 0.96597898 0.96185333
 0.95775884 0.95395333 0.95060521 0.94784337 0.94575524 0.94437236
 0.94367248 0.94358212 0.94399136 0.94476593 0.94576192 0.94683832
 0.94786692 0.94873846 0.94937044 0.94970691]
22 day output [[0.94971937]]
23 day input [0.87406179 0.88117296 0.87079206 0.86976567 0.86268125 0.85312948
 0.85693748 0.85375133 0.85542966 0.83636291 0.82274236 0.81823896
 0.82231777 0.8443567  0.85769975 0.85549318 0.86111659 0.87236006
 0.89641165 0.9128673  0.91561214 0.910303   0.91529453 0.92573895
 0.93115841 0.91733727 0.912319   0.90831375 0.90591661 0.91453894
 0.91870133 0.9101024  0.87439946 0.87035409 0.88253033 0.89454944
 0.89773225 0.89113929 0.86110322 0.85614178 0.85076578 0.85017067
 0.81891764 0.82935538 0.84306619 0.85657975 0.85196268 0.83781389
 0.83815825 0.85051169 0.85572053 0.89221583 0.9048267  0.90396079
 0.89423517 0.90090503 0.86765271 0.86257761 0.8811997  0.88668938
 0.90476986 0.87998275 0.86182202 0.86003002 0.88385093 0.91494348
 0.92898194 0.91894874 0.9481189  0.91663184 0.913439   0.9310414
 0.93790516 0.93624021 0.93901514 0.98818149 0.98166542 0.95937586
 0.96928298 0.97150922 0.96960205 0.96597898 0.96185333 0.95775884
 0.95395333 0.95060521 0.94784337 0.94575524 0.94437236 0.94367248
 0.94358212 0.94399136 0.94476593 0.94576192 0.94683832 0.94786692
 0.94873846 0.94937044 0.94970691 0.94971937]
23 day output [[0.9494059]]
24 day input [0.88117296 0.87079206 0.86976567 0.86268125 0.85312948 0.85693748
 0.85375133 0.85542966 0.83636291 0.82274236 0.81823896 0.82231777
 0.8443567  0.85769975 0.85549318 0.86111659 0.87236006 0.89641165
 0.9128673  0.91561214 0.910303   0.91529453 0.92573895 0.93115841
 0.91733727 0.912319   0.90831375 0.90591661 0.91453894 0.91870133
 0.9101024  0.87439946 0.87035409 0.88253033 0.89454944 0.89773225
 0.89113929 0.86110322 0.85614178 0.85076578 0.85017067 0.81891764
 0.82935538 0.84306619 0.85657975 0.85196268 0.83781389 0.83815825
 0.85051169 0.85572053 0.89221583 0.9048267  0.90396079 0.89423517
 0.90090503 0.86765271 0.86257761 0.8811997  0.88668938 0.90476986
 0.87998275 0.86182202 0.86003002 0.88385093 0.91494348 0.92898194
 0.91894874 0.9481189  0.91663184 0.913439   0.9310414  0.93790516
 0.93624021 0.93901514 0.98818149 0.98166542 0.95937586 0.96928298
 0.97150922 0.96960205 0.96597898 0.96185333 0.95775884 0.95395333
 0.95060521 0.94784337 0.94575524 0.94437236 0.94367248 0.94358212
 0.94399136 0.94476593 0.94576192 0.94683832 0.94786692 0.94873846
 0.94937044 0.94970691 0.94971937 0.94940591]
24 day output [[0.9487858]]
25 day input [0.87079206 0.86976567 0.86268125 0.85312948 0.85693748 0.85375133
 0.85542966 0.83636291 0.82274236 0.81823896 0.82231777 0.8443567
 0.85769975 0.85549318 0.86111659 0.87236006 0.89641165 0.9128673
 0.91561214 0.910303   0.91529453 0.92573895 0.93115841 0.91733727
 0.912319   0.90831375 0.90591661 0.91453894 0.91870133 0.9101024
 0.87439946 0.87035409 0.88253033 0.89454944 0.89773225 0.89113929
 0.86110322 0.85614178 0.85076578 0.85017067 0.81891764 0.82935538
 0.84306619 0.85657975 0.85196268 0.83781389 0.83815825 0.85051169
 0.85572053 0.89221583 0.9048267  0.90396079 0.89423517 0.90090503
 0.86765271 0.86257761 0.8811997  0.88668938 0.90476986 0.87998275
 0.86182202 0.86003002 0.88385093 0.91494348 0.92898194 0.91894874
 0.9481189  0.91663184 0.913439   0.9310414  0.93790516 0.93624021
 0.93901514 0.98818149 0.98166542 0.95937586 0.96928298 0.97150922
 0.96960205 0.96597898 0.96185333 0.95775884 0.95395333 0.95060521
 0.94784337 0.94575524 0.94437236 0.94367248 0.94358212 0.94399136
 0.94476593 0.94576192 0.94683832 0.94786692 0.94873846 0.94937044
 0.94970691 0.94971937 0.94940591 0.94878578]
25 day output [[0.94789547]]
26 day input [0.86976567 0.86268125 0.85312948 0.85693748 0.85375133 0.85542966
 0.83636291 0.82274236 0.81823896 0.82231777 0.8443567  0.85769975
 0.85549318 0.86111659 0.87236006 0.89641165 0.9128673  0.91561214
 0.910303   0.91529453 0.92573895 0.93115841 0.91733727 0.912319
 0.90831375 0.90591661 0.91453894 0.91870133 0.9101024  0.87439946
 0.87035409 0.88253033 0.89454944 0.89773225 0.89113929 0.86110322
 0.85614178 0.85076578 0.85017067 0.81891764 0.82935538 0.84306619
 0.85657975 0.85196268 0.83781389 0.83815825 0.85051169 0.85572053
 0.89221583 0.9048267  0.90396079 0.89423517 0.90090503 0.86765271
 0.86257761 0.8811997  0.88668938 0.90476986 0.87998275 0.86182202
 0.86003002 0.88385093 0.91494348 0.92898194 0.91894874 0.9481189
 0.91663184 0.913439   0.9310414  0.93790516 0.93624021 0.93901514
 0.98818149 0.98166542 0.95937586 0.96928298 0.97150922 0.96960205
 0.96597898 0.96185333 0.95775884 0.95395333 0.95060521 0.94784337
 0.94575524 0.94437236 0.94367248 0.94358212 0.94399136 0.94476593
 0.94576192 0.94683832 0.94786692 0.94873846 0.94937044 0.94970691
 0.94971937 0.94940591 0.94878578 0.94789547]
26 day output [[0.946785]]
27 day input [0.86268125 0.85312948 0.85693748 0.85375133 0.85542966 0.83636291
 0.82274236 0.81823896 0.82231777 0.8443567  0.85769975 0.85549318
 0.86111659 0.87236006 0.89641165 0.9128673  0.91561214 0.910303
 0.91529453 0.92573895 0.93115841 0.91733727 0.912319   0.90831375
 0.90591661 0.91453894 0.91870133 0.9101024  0.87439946 0.87035409
 0.88253033 0.89454944 0.89773225 0.89113929 0.86110322 0.85614178
 0.85076578 0.85017067 0.81891764 0.82935538 0.84306619 0.85657975
 0.85196268 0.83781389 0.83815825 0.85051169 0.85572053 0.89221583
 0.9048267  0.90396079 0.89423517 0.90090503 0.86765271 0.86257761
 0.8811997  0.88668938 0.90476986 0.87998275 0.86182202 0.86003002
 0.88385093 0.91494348 0.92898194 0.91894874 0.9481189  0.91663184
 0.913439   0.9310414  0.93790516 0.93624021 0.93901514 0.98818149
 0.98166542 0.95937586 0.96928298 0.97150922 0.96960205 0.96597898
 0.96185333 0.95775884 0.95395333 0.95060521 0.94784337 0.94575524
 0.94437236 0.94367248 0.94358212 0.94399136 0.94476593 0.94576192
 0.94683832 0.94786692 0.94873846 0.94937044 0.94970691 0.94971937
 0.94940591 0.94878578 0.94789547 0.94678497]
27 day output [[0.94551027]]
28 day input [0.85312948 0.85693748 0.85375133 0.85542966 0.83636291 0.82274236
 0.81823896 0.82231777 0.8443567  0.85769975 0.85549318 0.86111659
 0.87236006 0.89641165 0.9128673  0.91561214 0.910303   0.91529453
 0.92573895 0.93115841 0.91733727 0.912319   0.90831375 0.90591661
 0.91453894 0.91870133 0.9101024  0.87439946 0.87035409 0.88253033
 0.89454944 0.89773225 0.89113929 0.86110322 0.85614178 0.85076578
 0.85017067 0.81891764 0.82935538 0.84306619 0.85657975 0.85196268
 0.83781389 0.83815825 0.85051169 0.85572053 0.89221583 0.9048267
 0.90396079 0.89423517 0.90090503 0.86765271 0.86257761 0.8811997
 0.88668938 0.90476986 0.87998275 0.86182202 0.86003002 0.88385093
 0.91494348 0.92898194 0.91894874 0.9481189  0.91663184 0.913439
 0.9310414  0.93790516 0.93624021 0.93901514 0.98818149 0.98166542
 0.95937586 0.96928298 0.97150922 0.96960205 0.96597898 0.96185333
 0.95775884 0.95395333 0.95060521 0.94784337 0.94575524 0.94437236
 0.94367248 0.94358212 0.94399136 0.94476593 0.94576192 0.94683832
 0.94786692 0.94873846 0.94937044 0.94970691 0.94971937 0.94940591
 0.94878578 0.94789547 0.94678497 0.94551027]
28 day output [[0.94413006]]
29 day input [0.85693748 0.85375133 0.85542966 0.83636291 0.82274236 0.81823896
 0.82231777 0.8443567  0.85769975 0.85549318 0.86111659 0.87236006
 0.89641165 0.9128673  0.91561214 0.910303   0.91529453 0.92573895
 0.93115841 0.91733727 0.912319   0.90831375 0.90591661 0.91453894
 0.91870133 0.9101024  0.87439946 0.87035409 0.88253033 0.89454944
 0.89773225 0.89113929 0.86110322 0.85614178 0.85076578 0.85017067
 0.81891764 0.82935538 0.84306619 0.85657975 0.85196268 0.83781389
 0.83815825 0.85051169 0.85572053 0.89221583 0.9048267  0.90396079
 0.89423517 0.90090503 0.86765271 0.86257761 0.8811997  0.88668938
 0.90476986 0.87998275 0.86182202 0.86003002 0.88385093 0.91494348
 0.92898194 0.91894874 0.9481189  0.91663184 0.913439   0.9310414
 0.93790516 0.93624021 0.93901514 0.98818149 0.98166542 0.95937586
 0.96928298 0.97150922 0.96960205 0.96597898 0.96185333 0.95775884
 0.95395333 0.95060521 0.94784337 0.94575524 0.94437236 0.94367248
 0.94358212 0.94399136 0.94476593 0.94576192 0.94683832 0.94786692
 0.94873846 0.94937044 0.94970691 0.94971937 0.94940591 0.94878578
 0.94789547 0.94678497 0.94551027 0.94413006]
29 day output [[0.9426998]]
[[0.9593758583068848], [0.9692829847335815], [0.9715092182159424], [0.9696020483970642], [0.9659789800643921], [0.9618533253669739], [0.9577588438987732], [0.9539533257484436], [0.9506052136421204], [0.9478433728218079], [0.9457552433013916], [0.9443723559379578], [0.9436724781990051], [0.9435821175575256], [0.9439913630485535], [0.9447659254074097], [0.9457619190216064], [0.9468383193016052], [0.9478669166564941], [0.9487384557723999], [0.9493704438209534], [0.9497069120407104], [0.9497193694114685], [0.9494059085845947], [0.9487857818603516], [0.9478954672813416], [0.9467849731445312], [0.9455102682113647], [0.9441300630569458], [0.9426997900009155]]
day_new=np.arange(1,101)
day_pred=np.arange(101,131)
import matplotlib.pyplot as plt
len(df1)
1258
plt.plot(day_new,scaler.inverse_transform(df1[1158:]))
plt.plot(day_pred,scaler.inverse_transform(lst_output))
[<matplotlib.lines.Line2D at 0x20de86405e0>]

 
df3=df1.tolist()
df3.extend(lst_output)
plt.plot(df3[1200:])
[<matplotlib.lines.Line2D at 0x20de866d4c0>]

df3=scaler.inverse_transform(df3).tolist()
plt.plot(df3)
[<matplotlib.lines.Line2D at 0x20de86911f0>]

 
